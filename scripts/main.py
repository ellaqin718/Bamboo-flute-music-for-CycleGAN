from __future__ import print_function, division
from glob import glob
import soundfile as sf
import numpy as np
import matplotlib.pyplot as plt
import librosa
import librosa.display
import IPython
from torch.autograd import Variable
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tensordot_pytorch import tensordot_pytorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
from torchaudio.transforms import MelScale, Spectrogram
import warnings
#数据集不同；hop size不同，根据音乐长度自由设置；采样率根据音乐采样速率调整
warnings.filterwarnings("ignore")

# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device = "cpu"
# Hyperparameters
# 两个相邻窗口之间错开的sample数,越小,则说明时序解析度越高,计算成本也越高
hop = 150  # hop size (window size = 6*hop)

# 采样率，跟音乐的采样速率相关
#
sr = 16000  # sampling rate

# 正则化的下限
min_level_db = -15  #

# 正则化的上限
ref_level_db = 15

# 生成器的分割样条图的时间轴长度 与模型输入契合 经验值
shape = 24  # length of time axis of split specrograms to feed to generator

# siamese vector的矢量长度
vec_len = 128  # length of vector generated by siamese vector

# 批训练数量
bs = 16  # batch size

delta = 2.  # constant for siamese loss

# https://blog.csdn.net/weixin_43335465/article/details/126016734
# 求输入信号的语谱图
specobj = Spectrogram(n_fft=6 * hop, win_length=6 * hop, hop_length=hop, pad=0, power=2, normalized=True)
specfunc = specobj.forward
# 将stft的结果转换为mel域中的stft的结果
melobj = MelScale(n_mels=hop, sample_rate=sr, f_min=0.)
melfunc = melobj.forward

def melspecfunc(waveform):
    specgram = specfunc(waveform)
    mel_specgram = melfunc(specgram)
    return mel_specgram


# 光谱融合
# def spectral_convergence(input, target):
#     return 20 * ((input - target).norm().log10() - target.norm().log10())


def GRAD(spec, transform_fn, samples=None, init_x0=None, maxiter=100, tol=1e-6, verbose=1, evaiter=10, lr=0.003):
    spec = torch.Tensor(spec).to('cpu')
    samples = (spec.shape[-1] * hop) - hop

    if init_x0 is None:
        init_x0 = spec.new_empty((1, samples)).normal_(std=1e-6)
    x = nn.Parameter(init_x0)
    T = spec

    criterion = nn.L1Loss()

    optimizer = torch.optim.Adam([x], lr=lr)

    bar_dict = {}

    bar_dict['spectral_convergence'] = 0

    for i in range(maxiter):
        optimizer.zero_grad()
        V = transform_fn(x)
        loss = criterion(V, T)

        if not torch.any(torch.isnan(loss)):
            loss.backward()
            optimizer.step()

        lr = lr * 0.9999
        for param_group in optimizer.param_groups:
            optimizer.lr = lr

        if i % evaiter == evaiter - 1:
            with torch.no_grad():
                V = transform_fn(x)

    return x.detach().view(-1).cpu()


def normalize(S):
    return np.clip((((S - min_level_db) / -min_level_db) * 2.) - 1., -1, 1)


def denormalize(S):
    return (((np.clip(S, -1, 1) + 1.) / 2.) * -min_level_db) + min_level_db


def prep(wv, hop=192):
    S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1, -1))).detach().cpu())
    S = librosa.power_to_db(S) - ref_level_db
    return normalize(S)


def deprep(S):
    S = denormalize(S) + ref_level_db
    S = librosa.db_to_power(S)
    wv = GRAD(np.expand_dims(S, 0), melspecfunc, maxiter=100, evaiter=10,
              tol=1e-8)  ##MAXITER NORMALLY 2000 BUT SET TO 100 FOR TESTING
    print("wv shape: ", wv.shape)
    return np.array(np.squeeze(wv))


# 从波形阵列生成谱图
# Generate spectrograms from waveform array
def tospec(data):
    print("DATA LEN:", len(data))
    specs = np.empty(len(data), dtype=object)
    for i in range(len(data)):
        x = data[i]
        S = prep(x)
        S = np.array(S, dtype=np.float32)
        specs[i] = np.expand_dims(S, -1)
    return specs


# 从单个wav文件生成多个确定长度的谱图
# Generate multiple spectrograms with a determined length from single wav file
def tospeclong(path, length=4 * 16000):
    x, sr = librosa.load(path, sr=16000)
    x, _ = librosa.effects.trim(x)
    loudls = librosa.effects.split(x, top_db=50)
    xls = np.array([])
    for interv in loudls:
        xls = np.concatenate((xls, x[interv[0]:interv[1]]))
    x = xls
    num = x.shape[0] // length
    specs = np.empty(num, dtype=object)
    for i in range(num - 1):
        a = x[i * length:(i + 1) * length]
        S = prep(a)
        S = np.array(S, dtype=np.float32)
        try:
            sh = S.shape
            specs[i] = S
        except AttributeError:
            print('spectrogram failed')
    return specs


# 波形阵列来自包含wav文件的文件夹路径
def audio_array(path):
    ls = glob(f'{path}/*.wav')
    adata = []
    for i in range(len(ls)):
        x, sr = torchaudio.load(ls[i])

        x = np.array(x, dtype=np.float32)
        adata.append(x)
    return adata


# 将光谱图分成大小相等的块
def splitcut(data):
    ls = []
    mini = 0
    minifinal = 10 * shape
    for i in range(data.shape[0] - 1):
        if data[i].shape[1] <= data[i + 1].shape[1]:
            mini = data[i].shape[1]
        else:
            mini = data[i + 1].shape[1]
        if mini >= 3 * shape and mini < minifinal:
            minifinal = mini
    for i in range(data.shape[0]):
        x = data[i]
        if x.shape[1] >= 3 * shape:
            for n in range(x.shape[1] // minifinal):
                ls.append(x[:, n * minifinal:n * minifinal + minifinal, :])
            ls.append(x[:, -minifinal:, :])
    return np.array(ls)


# flute MUSIC
awv = audio_array('musicprocess/output/bambooflute_wav')
aspec = tospec(awv)
adata = splitcut(aspec)

# flute_long MUSIC
bwv = audio_array('musicprocess/output/flute_wav')
bspec = tospec(bwv)
bdata = splitcut(bspec)

class AudioDataset(Dataset):
    def __init__(self, data, hop, shape):
        self.data = torch.Tensor(data).permute(0, 3, 1, 2)
        self.hop = hop
        self.shape = shape

    def __getitem__(self, idx):
        return self.data[idx, :, :, :3 * self.shape].to(device)

    def __len__(self):
        return len(self.data)


a_dataset = AudioDataset(adata, hop=hop, shape=shape)

# 1414
# print(a_dataset)

# 1414/16=89
a_loader = DataLoader(dataset=a_dataset, batch_size=bs, shuffle=True)

b_dataset = AudioDataset(bdata, hop=hop, shape=shape)

b_loader = DataLoader(dataset=b_dataset, batch_size=bs, shuffle=True)

# 用于二维图像或音乐的卷积处理
class ConvSN2D(nn.Conv2d):
    def __init__(self, in_channels, filters, kernel_size, strides, padding='same', power_iterations=1):

        # super(ConvSN2D, self).__init__(in_channels=in_channels, out_channels=filters, kernel_size=kernel_size,
        #                                stride=strides, padding=padding)
        super(ConvSN2D, self).__init__(in_channels=in_channels, out_channels=filters, kernel_size=kernel_size,
                                       stride=strides)
        self.power_iterations = power_iterations
        self.strides = strides
        self.padding = padding
        self.filters = filters
        self.kernel_size = kernel_size

        self.u = torch.nn.Parameter(data=torch.zeros((1, self.weight.shape[-1]))
                                    , requires_grad=False)
        torch.nn.init.normal_(self.u.data, mean=0.5, std=0.5)

        self.u.data.uniform_(0, 1)

    def compute_spectral_norm(self, W, new_u, W_shape):
        for _ in range(self.power_iterations):
            new_v = F.normalize(torch.matmul(new_u, torch.transpose(W, 0, 1)), p=2)
            new_u = F.normalize(torch.matmul(new_v, W), p=2)
            # new_v = l2normalize(torch.matmul(new_u, torch.transpose(W)))
            # new_u = l2normalize(torch.matmul(new_v, W))

        sigma = torch.matmul(W, torch.transpose(new_u, 0, 1))
        W_bar = W / sigma

        self.u = torch.nn.Parameter(data=new_u)
        W_bar = W_bar.reshape(W_shape)

        return W_bar

    def forward(self, inputs):
        W_shape = self.weight.shape
        W_reshaped = self.weight.reshape((-1, W_shape[-1]))
        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)

        if self.padding == 'same':
            stride_h, stride_w = self.strides if isinstance(self.strides, tuple) else [self.strides, self.strides]
            pad_h = ((inputs.shape[2] - 1) * stride_h - inputs.shape[2] + self.kernel_size[0]) // 2
            pad_w = ((inputs.shape[3] - 1) * stride_w - inputs.shape[3] + self.kernel_size[1]) // 2
        else:
            pad_h, pad_w = 0, 0

        outputs = F.conv2d(inputs, new_kernel, stride=self.strides, padding=(pad_h, pad_w))

        return outputs

# https://blog.csdn.net/wind82465/article/details/108770753
# 反卷积操作
class ConvSN2DTranspose(nn.ConvTranspose2d):
    def __init__(self, in_channels, filters, kernel_size, power_iterations=1, strides=2, padding='valid'):
        super(ConvSN2DTranspose, self).__init__(in_channels=in_channels, out_channels=filters, kernel_size=kernel_size,
                                                stride=strides, padding=padding)
        self.power_iterations = power_iterations
        self.strides = strides
        self.filters = filters
        self.kernel_size = kernel_size
        self.padding = padding

        self.u = torch.nn.Parameter(data=torch.zeros((1, self.weight.shape[-1]))
                                    , requires_grad=False)
        torch.nn.init.normal_(self.u.data, mean=0.5, std=0.5)

        self.u.data.uniform_(0, 1)

    def compute_spectral_norm(self, W, new_u, W_shape):
        for _ in range(self.power_iterations):
            new_v = F.normalize(torch.matmul(new_u, torch.transpose(W, 0, 1)), p=2)
            new_u = F.normalize(torch.matmul(new_v, W), p=2)

        sigma = torch.matmul(W, torch.transpose(new_u, 0, 1))
        W_bar = W / sigma

        self.u = torch.nn.Parameter(data=new_u)
        W_bar = W_bar.reshape(W_shape)

        return W_bar

    def forward(self, inputs):

        W_shape = self.weight.shape
        W_reshaped = self.weight.reshape((-1, W_shape[-1]))
        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)

        if self.padding == 'same':
            stride_h, stride_w = self.strides if isinstance(self.strides, tuple) else [self.strides, self.strides]
            pad_h = ((inputs.shape[2] - 1) * stride_h - hop + self.kernel_size[0]) // 2
            pad_w = ((inputs.shape[3] - 1) * stride_w - 24 + self.kernel_size[1]) // 2
            # Here we are very cheekily forcing output shape...
        else:
            pad_h, pad_w = 0, 0



        outputs = F.conv_transpose2d(
            inputs,
            new_kernel,
            None,
            stride=self.strides,
            padding=(pad_h, pad_w))

        # CODE FOR BIAS AND ACTIVATION FN HERE  
        return outputs


# 全连接层
class DenseSN(nn.Linear):
    def __init__(self, input_shape):
        super(DenseSN, self).__init__(in_features=input_shape, out_features=1)

        self.u = torch.nn.Parameter(data=torch.zeros((1, self.weight.shape[-1]))
                                    , requires_grad=False)
        torch.nn.init.normal_(self.u.data, mean=0.5, std=0.5)

        self.u.data.uniform_(0, 1)

    def compute_spectral_norm(self, W, new_u, W_shape):
        new_v = F.normalize(torch.matmul(new_u, torch.transpose(W, 0, 1)), p=2)
        new_u = F.normalize(torch.matmul(new_v, W), p=2)

        sigma = torch.matmul(W, torch.transpose(new_u, 0, 1))
        W_bar = W / sigma

        self.u = torch.nn.Parameter(data=new_u)
        W_bar = W_bar.reshape(W_shape)

        return W_bar

    def forward(self, inputs):
        W_shape = self.weight.shape
        W_reshaped = self.weight.reshape((-1, W_shape[-1]))
        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)

        rank = len(inputs.shape)

        if rank > 2:
            # Thanks to deanmark on GitHub for pytorch tensordot function
            outputs = tensordot_pytorch(inputs, new_kernel, [[rank - 1], [0]])
        else:
            outputs = torch.matmul(inputs, torch.transpose(new_kernel, 0, 1))

        # CODE FOR BIAS AND ACTIVATION FN HERE 
        return outputs


# 提取功能:分光图拆分
def extract_image(im):
    shape = im.shape
    height = shape[2]
    width = shape[3]

    im1 = im[:, :, :, 0:(width - (2 * width // 3))]
    im2 = im[:, :, :, width // 3:(width - (width // 3))]
    im3 = im[:, :, :, (2 * width // 3):width]

    return im1, im2, im3


# 组装功能:串联谱图
def assemble_image(lsim):
    im1, im2, im3 = lsim

    imh = torch.cat((im1, im2, im3), dim=3)

    return imh


class Generator(nn.Module):
    def __init__(self, input_shape):
        super(Generator, self).__init__()

        h, w, c = input_shape

        self.leaky = nn.LeakyReLU(0.2, inplace=True)
        self.batchnorm = nn.BatchNorm2d(num_features=256)

        # downscaling
        # self.g0 = nn.ConstantPad2d((0,1), 0)
        self.g1 = ConvSN2D(in_channels=c, filters=256, kernel_size=(h, 3), strides=1, padding='valid')
        self.g2 = ConvSN2D(in_channels=256, filters=256, kernel_size=(1, 9), strides=(1, 2))
        self.g3 = ConvSN2D(in_channels=256, filters=256, kernel_size=(1, 7), strides=(1, 2))

        # upscaling
        self.g4 = ConvSN2D(in_channels=256, filters=256, kernel_size=(1, 7), strides=(1, 1))
        self.g5 = ConvSN2D(in_channels=256, filters=256, kernel_size=(1, 9), strides=(1, 1))

        self.g6 = ConvSN2DTranspose(in_channels=256, filters=1, kernel_size=(h, 2), strides=(1, 1), padding='same')

    def forward(self, x):
        # NOTE: YET TO IMPLEMENT BATCH NORM, ACTIVATION FUNCTIONS, RELU ETC
        # downscaling
        x1 = self.g1(x)
        x1 = self.batchnorm(self.leaky(x1))
        x2 = self.g2(x1)
        x2 = self.batchnorm(self.leaky(x2))
        x3 = self.g3(x2)
        x3 = self.batchnorm(self.leaky(x3))

        # upscaling
        x4 = F.interpolate(x3, size=(x3.shape[2], x3.shape[3] * 2))
        x = self.g4(x4)
        x = self.batchnorm(self.leaky(x))
        x = torch.cat((x, x3), dim=3)

        x = F.interpolate(x, size=(x.shape[2], x.shape[3] * 2))
        x = self.g5(x)
        x = self.leaky(x)
        x = torch.cat((x, x2), dim=3)

        x = torch.tanh(self.g6(x))

        return x


class Siamese(nn.Module):
    def __init__(self, input_shape):
        super(Siamese, self).__init__()

        h, w, c = input_shape
        self.leaky = nn.LeakyReLU(0.2, inplace=True)
        self.batchnorm = nn.BatchNorm2d(num_features=256)

        self.g1 = nn.Conv2d(in_channels=c, out_channels=256, kernel_size=(h, 9), stride=1, padding=0)
        self.g2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 9), stride=(1, 2))
        self.g3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 7), stride=(1, 2))
        self.g4 = nn.Linear(1536, 128)

    def forward(self, x):
        # We have to define layers on the fly in order to ensure 'same' padding

        x = self.g1(x)
        x = self.batchnorm(self.leaky(x))

        # New stride = (1,2)
        # New kernel = (1,9)
        pad_h = ((x.shape[2] - 1) * 1 - x.shape[2] + 1) // 2
        pad_w = ((x.shape[3] - 1) * 2 - x.shape[3] + 9) // 2
        x = F.pad(x, (pad_h, pad_w))
        x = self.g2(x)
        x = self.batchnorm(self.leaky(x))

        # New stride = (1,2)
        # New kernel = (1,7)
        pad_h = ((x.shape[2] - 1) * 1 - x.shape[2] + 1) // 2
        pad_w = ((x.shape[3] - 1) * 2 - x.shape[3] + 7) // 2
        x = F.pad(x, (pad_h, pad_w))
        x = self.g3(x)
        x = self.batchnorm(self.leaky(x))

        x = x.view(x.shape[0], -1)
        x = self.g4(x)
        return x


class Discriminator(nn.Module):
    def __init__(self, input_shape):
        super(Discriminator, self).__init__()

        h, w, c = input_shape

        self.leaky = nn.LeakyReLU(0.2, inplace=True)

        self.g1 = ConvSN2D(in_channels=c, filters=512, kernel_size=(h, 3), strides=1, padding='valid')
        self.g2 = ConvSN2D(in_channels=512, filters=512, kernel_size=(1, 9), strides=(1, 2))
        self.g3 = ConvSN2D(in_channels=512, filters=512, kernel_size=(1, 7), strides=(1, 2))
        self.g4 = DenseSN(input_shape=35328)

    def forward(self, x):
        x = self.g1(x)
        x = self.leaky(x)
        x = self.g2(x)
        x = self.leaky(x)
        x = self.g3(x)
        x = self.leaky(x)
        x = self.g4(x.view(x.shape[0], -1))
        return x

    # 生成一个随机批处理来显示当前的培训结果

# ===========DEFINE LOSS FUNCS =============#

def mae(x, y):
    return torch.mean(torch.abs(x - y))

def mse(x, y):
    return torch.mean((x - y) ** 2)


def loss_travel(sa, sab, sa1, sab1):

    l1 = torch.mean(((sa - sa1) - (sab - sab1)) ** 2)
    l2 = torch.mean(torch.sum(-(F.normalize(sa - sa1, p=2, dim=-1) *
                                F.normalize(sab - sab1, dim=-1)), dim=-1))
    return l1 + l2

def loss_siamese(sa, sa1):
    logits = torch.sqrt(torch.sum(((sa - sa1) ** 2), axis=-1, keepdim=True))
    return Variable(torch.mean(torch.max((delta - logits), 0)[0] ** 2), requires_grad=True)


def d_loss_f(fake):
    return Variable(torch.mean(torch.max(1 + fake, 0)[0]), requires_grad=True)


def d_loss_r(real):
    return Variable(torch.mean(torch.max(1 - real, 0)[0]), requires_grad=True)


def g_loss_f(fake):
    return torch.mean(-fake)


# =======SET UP MODELS AND OPTIMIZERS =======#
gen = Generator((hop, shape, 1)).to(device)

siam = Siamese((hop, shape, 1)).to(device)

critic = Discriminator((hop, 3 * shape, 1)).to(device)

# 同时优化两个模型
params = list(gen.parameters()) + list(siam.parameters())

opt_gen = optim.Adam(params, lr=1e-5)

opt_disc = optim.SGD(critic.parameters(), lr=1e-5)

# 学习率设置
def update_lr(gen_lr, dis_lr):
    opt_gen.lr = gen_lr
    opt_disc.lr = dis_lr


def train_all(a, b):
    # 分光图分为三部分
    aa, aa2, aa3 = extract_image(a)

    opt_gen.zero_grad()

    # translating A to B
    fab = gen.forward(aa)
    fab2 = gen.forward(aa2)
    fab3 = gen.forward(aa3)

    # concatenate/assemble converted spectrograms
    fabtot = assemble_image([fab, fab2, fab3])

    # feed concatenated spectrograms to critic
    cab = critic.forward(fabtot)
    cb = critic.forward(b)

    # feed 2 pairs (A,G(A)) extracted spectrograms to Siamese
    sab = siam.forward(fab)
    sab2 = siam.forward(fab3)
    sa = siam.forward(aa)
    sa2 = siam.forward(aa3)

    # travel loss
    loss_travel_temp = loss_travel(sa, sab, sa2, sab2)
    loss_siamese_temp = loss_siamese(sa, sa2)
    loss_m = loss_travel_temp + loss_siamese_temp

    loss_g = g_loss_f(cab)

    lossgtot = loss_g + 10. * loss_m  # +0.5*loss_id #CHANGE LOSS WEIGHTS HERE  (COMMENT OUT +w*loss_id IF THE IDENTITY LOSS TERM IS NOT NEEDED)

    if not torch.any(torch.isnan(lossgtot)):
        lossgtot.backward()
        opt_gen.step()

    # get critic loss and bptt
    opt_disc.zero_grad()

    loss_dr = d_loss_r(cb)
    loss_df = d_loss_f(cab)
    loss_d = (loss_dr + loss_df) / 2.

    if not torch.any(torch.isnan(loss_d)):
        loss_d.backward()
        opt_disc.step()

    return loss_dr, loss_df, loss_g, loss_d


def train_d(a, b):
    opt_disc.zero_grad()

    aa, aa2, aa3 = extract_image(a)

    # translating A to B
    fab = gen.forward(aa)
    fab2 = gen.forward(aa2)
    fab3 = gen.forward(aa3)

    # concatenate/assemble converted spectrograms
    fabtot = assemble_image([fab, fab2, fab3])

    # feed concatenated spectrograms to critic
    cab = critic.forward(fabtot)
    cb = critic.forward(b)

    # get critic loss and bptt
    loss_dr = d_loss_r(cb)
    loss_df = d_loss_f(cab)
    loss_d = (loss_dr + loss_df) / 2.
    if not torch.any(torch.isnan(loss_d)):
        loss_d.backward()
        opt_disc.step()


    return loss_dr, loss_df

def train(epochs, batch_size=16, gen_lr=1e-5, dis_lr=1e-5, n_save=6, gupt=5):
    update_lr(gen_lr, dis_lr)

    df_list = []
    dr_list = []
    g_list = []
    id_list = []

    g = 0

    loss_list = []

    for epoch in range(epochs):
        # 89//20=4
        # 0-88
        for batchi, (a, b) in enumerate(zip(a_loader, b_loader)):

            # 每隔gupt次训练一次生成器
            if batchi % gupt == 0:
                dloss_t, dloss_f, gloss, idloss = train_all(a, b)
            else:
                dloss_t, dloss_f = train_d(a, b)

            loss = dloss_t + dloss_f

            loss_list.append(loss.cpu().detach().item())

            df_list.append(dloss_f.detach().item())

            dr_list.append(dloss_t)

            g_list.append(gloss.detach().item())

            id_list.append(idloss)

            g += 1

            if (batchi+1) % 20 == 0:

                print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [gloss {sum(g_list[-g:])/g}] [D loss {sum(df_list[-g:])/g}] ')
                # print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [D loss f: {np.mean(df_list[-g:], axis=0)} ', end='')
                # print(f'r: {np.mean(dr_list[-g:], axis=0)}] ', end='')
                # print(f'[G loss: {np.mean(g_list[-g:], axis=0)}] ', end='')
                # print(f'[ID loss: {np.mean(id_list[-g:])}] ', end='')
                g = 0

    return g_list, df_list, loss_list

g_list, df_list, loss_list = train(epochs=100, batch_size=bs, gen_lr=1e-7, dis_lr=1e-8)

# 保存模型
torch.save(gen.state_dict(), "gen.ckpt")
import pickle

with open("g_list.pkl", "wb") as f:
    pickle.dump(g_list, f)

with open("loss_list.pkl", "wb") as f:
    pickle.dump(loss_list, f)

#file


# jiazai

##
plt.figure(figsize=(8, 6), dpi=300)
plt.plot([i+1 for i in range(len(loss_list))], loss_list, linewidth=2, label="Discriminator Loss", c="#009933")
plt.plot([i+1 for i in range(len(loss_list))], g_list, linewidth=2, label="Generator Loss", c="#0000FF")

plt.ylabel("Loss", fontsize=16, weight='semibold')
plt.yticks(fontsize=14, weight='semibold')
plt.xticks(fontsize=14, weight='semibold')
plt.xlabel("Time", fontsize=16, weight='semibold')
plt.legend(fontsize=14, prop={'size': 14, 'weight':'semibold'})
plt.savefig("loss.png")

plt.show()

plt.figure(figsize=(8, 6), dpi=300)
plt.plot([i+1 for i in range(len(loss_list))], loss_list, linewidth=2, label="Discriminator Loss", c="#009933")

plt.ylabel("Loss", fontsize=16, weight='semibold')
plt.yticks(fontsize=14, weight='semibold')
plt.xticks(fontsize=14, weight='semibold')
plt.xlabel("Time", fontsize=16, weight='semibold')
plt.legend(fontsize=14, prop={'size': 14, 'weight':'semibold'})
plt.savefig("loss1.png")

plt.show()
#
plt.figure(figsize=(8, 6), dpi=300)
plt.plot([i+1 for i in range(len(loss_list))], g_list, linewidth=2, label="Generator Loss", c="#0000FF")

plt.ylabel("Loss", fontsize=16, weight='semibold')
plt.yticks(fontsize=14, weight='semibold')
plt.xticks(fontsize=14, weight='semibold')
plt.xlabel("Time", fontsize=16, weight='semibold')
plt.legend(fontsize=14, prop={'size': 14, 'weight':'semibold'})
plt.savefig("loss2.png")

plt.show()


# After Training, use these functions to convert data with the generator and save the results

# Assembling generated Spectrogram chunks into final Spectrogram

def specass(a, spec):
    but = False
    con = np.array([])
    nim = a.shape[0]
    for i in range(nim - 1):
        im = a[i]
        im = np.squeeze(im)
        if not but:
            con = im
            but = True
        else:
            con = np.concatenate((con, im), axis=1)
    diff = spec.shape[1] - (nim * shape)
    a = np.squeeze(a)
    con = np.concatenate((con, a[-1, :, -diff:]), axis=1)
    return np.squeeze(con)


# Splitting input spectrogram into different chunks to feed to the generator
def chopspec(spec):
    dsa = []
    for i in range(spec.shape[1] // shape):
        im = spec[:, i * shape:i * shape + shape]
        im = np.reshape(im, (im.shape[0], im.shape[1], 1))
        dsa.append(im)
    imlast = spec[:, -shape:]
    imlast = np.reshape(imlast, (imlast.shape[0], imlast.shape[1], 1))
    dsa.append(imlast)
    return np.array(dsa, dtype=np.float32)


# Converting from source Spectrogram to target Spectrogram
def towave(spec, name, path='output/', show=True):

    specarr = chopspec(spec)

    a = torch.Tensor(specarr).permute(0, 3, 1, 2).to(device)

    ab = gen(a).detach().cpu().numpy()

    a = specass(a.cpu(), spec)

    ab = specass(ab, spec)

    print("out of specass")

    awv = deprep(a)

    abwv = deprep(ab)

    print("out of deprep")

    sf.write('AB_' + str(name) + '.wav', abwv, sr)

    sf.write('A_' + str(name) + '.wav', awv, sr)

    IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))
    IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))
    if show:
        fig, axs = plt.subplots(ncols=2)
        axs[0].imshow(np.flip(a, -2), cmap=None)
        axs[0].axis('off')
        # axs[0].set_title('Source')
        axs[1].imshow(np.flip(ab, -2), cmap=None)
        axs[1].axis('off')
        # axs[1].set_title('Generated')
        plt.savefig("show.png", dpi=300)
        plt.show()
    return abwv

# Wav to wav conversion
print("over")

file_path = 'data/flute/flute0.wav'
wv, sr = librosa.load(file_path, sr=16000)
speca = prep(wv)
plt.figure(figsize=(50, 1), dpi=300)
plt.imshow(np.flip(speca, axis=0), cmap=None)
plt.axis('off')
plt.savefig("picture.png")
plt.show()
abwv = towave(speca, name=0, path='output')  # Convert and save wav

